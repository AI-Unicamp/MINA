# -*- coding: utf-8 -*-
"""ddo-estudo.ipynb

Automatically generated by Colaboratory.

# Introdução e Importação da Base

Neste arquivo tentarei reproduzir o Projeto de Detecção de Discurso de Ódio

**Importações**

**NLTK** - Natural Language Toolkit é uma biblioteca para Processamento de Linguagem Natural que utiliza o Python

**Scikit-Learn** - is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities.

**pandas** - biblioteca que se utiliza da linguagem Python para manipular e analisar dados: manipulação de *dataframes*
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

from string import punctuation
from nltk.probability import FreqDist
from wordcloud import WordCloud

from sklearn.svm import LinearSVC
from sklearn.svm import SVC
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score, cross_validate
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.model_selection import GridSearchCV

import matplotlib.pyplot as plt
import numpy as np
import re

# %load_ext google.colab.data_table

"""**Trazer a base de dados**

Comando **!git clone** copia os documentos do github e cria uma pasta dentro do colab com os arquivos
"""

!git clone https://github.com/eduardofroes/OffComBR.git

data_frame = pd.read_csv('./OffComBR/OffComBR.csv', delimiter=';', encoding='utf-8', header = None, names = ['Label', 'Data'], error_bad_lines = True, warn_bad_lines = True)
dff = pd.DataFrame(data_frame)
dff

"""Quantas amostras têm *label* **sim** e quantas tem *label* **não** ?"""

yes = len(data_frame[data_frame.Label == 'yes'])
no = len(data_frame[data_frame.Label == 'no'])

print("Amostras Classificadas como yes:", yes)
print("Amostras Classificadas como no:", no)

"""# Pré-processamento dos Dados e Análise dos Data Frames

## Tokenization
"""

nltk.download('stopwords')
nltk.download('punkt')

#Tokenize the Data Frame

tokenized_data_frame = data_frame['Data'].apply(word_tokenize)

#Tokenize Hate Data Frame

hate_data_frame = data_frame.loc[data_frame['Label']=='yes']

tokenized_hate_data_frame = hate_data_frame['Data'].apply(word_tokenize)

#Tokenize No Hate Data Frame

no_hate_data_frame = data_frame.loc[data_frame['Label']=='no']

tokenized_no_hate_data_frame = no_hate_data_frame['Data'].apply(word_tokenize)

"""## Nuvens e Gráficos Sem Pré-Processamento

**Nuvem de Palavras**
"""

def plotarNuvem(data_tokenized):
  allWordsinRows = data_tokenized[:][:]
  allWords = []

  for words in allWordsinRows:
    allWords.extend(words)

  # The FreqDist class is used to encode “frequency distributions”, which count
  # the number of times that each outcome of an experiment occurs
  # FreqDist devolve um dicionário onde cada key é uma palavra distinta (?)

  frequencia = nltk.FreqDist(allWords)
  sorted(frequencia, key = frequencia.__getitem__, reverse = True)

  wordcloud = WordCloud().generate_from_frequencies(frequencia)
  plt.figure(figsize=(10,10))
  plt.imshow(wordcloud)
  plt.axis("off")
  plt.show()

  return allWordsinRows, frequencia

"""**Gráfico de Frequências**"""

def plotFrenquencies(fr):
  fr.plot(20,cumulative=False)

#Palavras com maior frequência no dataframe sem retirada de stopwords
allWords_data_1, freq_data_1 = plotarNuvem(tokenized_data_frame)
plotFrenquencies(freq_data_1)

#Palavras com maior frequência no dataframe HATE

allWords_hate, freq_hate = plotarNuvem(tokenized_hate_data_frame)
plotFrenquencies(freq_hate)

#Palavras com maior frequência no dataframe NO HATE

allWords_nohate, freq_nohate = plotarNuvem(tokenized_no_hate_data_frame)
plotFrenquencies(freq_nohate)

"""Nota-se que as palavras de maior frequência que aparecem nos três *dataframes* são muito parecidas e são conectivos. Isto é palavras que, geralmente, não acrescentam muita informação na frase. Em seguida, essas palavras serão consideradas *stopwords* e serão retiradas do *dataset*

## Removendo pontuação, *stopwords*, ...

**Conceitos sobre pré-processamento de dados:**

*removing pontuaction* 
- a remoção de pontuação pode ser feita retirando a pontuação por completo ou substituindo-a por outro caractere como o espaço. Ambas as formas possuem desvantagens:

       it's -> it s (o 's' torna-se uma palavra do vocabulário da base: pode ser preciso removê-la posteriormente como uma *stopword*)

      Ireland-related -> Irelandrelated (x)

*stopwords* 
- é uma lista de palavras que vão ser desconsideradas. Geralmente, são palavras que possuem um significado linguístico/gramatical mas não agregam muito conteúdo para o significado da frase. Além disso podem ser agregadas à lista palavras que no contexto do *dataframe* estudado possuem uma frequência muito alta de forma que, em problemas de classificação, elas não ajudem a diferenciar entre as classes. 
- retirar palavras menores do que um determinado tamanho

*altering the text* 
- palavras iguais mas que estão com letra maiuscula/minuscula serão consideradas palavras diferentes no vocabulário se não forem corrigidas. Essa correção deve ser feita com cuidado pois podem existir situações em que se queira ter essa distinção.
- pode-se tentar corrigir *typos*

*stemming* 
- palavras ligeiramente diferentes mas que possuem o mesmo significado podem ser substituidas pela mesma palavra
      andamos, andar, andei, andarei -> andar
- *Porter Algorithm*

É importante ressaltar que a ordem com que esses passos são realizados muitas vezes altera o resultado final do *dataframe* e, portanto, a estratégia de processamento deve ser previamente definida ou testada de diferentes formas.
"""

#Retirando pontuações e stopwords pré-definidas para a língua portuguesa pelo nltk

stopwords_1 = set(stopwords.words('portuguese')+list(punctuation))
print(stopwords_1)

def removeStopWords(text):
  #text.lower -> transforma todas as letras em minúculas
  #text ainda não foi tokenizado
  filteredWords = [word for word in word_tokenize(text.lower()) if word not in stopwords_1]
  return [word for word in filteredWords if len(word) > 1]

tokenized_data_frame_1 = data_frame[:]['Data'].apply(removeStopWords)

tokenized_hate_data_frame_1 = hate_data_frame['Data'].apply(removeStopWords)

tokenized_no_hate_data_frame_1 = no_hate_data_frame['Data'].apply(removeStopWords)

"""**Gráficos**"""

#Palavras com maior frequência no dataframe retirando stopwords (pontuação e lista pt)
allWords_data_basico, freq_data_basico = plotarNuvem(tokenized_data_frame_1)
plotFrenquencies(freq_data_basico)

#Palavras com maior frequência no dataframe hate retirando stopwords (pontuação e lista pt)
allWords_data, freq_data = plotarNuvem(tokenized_hate_data_frame_1)
plotFrenquencies(freq_data)

#Palavras com maior frequência no dataframe no hate retirando stopwords (pontuação e lista pt)
allWords_data, freq_data = plotarNuvem(tokenized_no_hate_data_frame_1)
plotFrenquencies(freq_data)

"""Percebe-se que as palavras de maior frequência foram retiradas, mas que as palavras de maior frequência ainda são as mesmas para os *dataframes* de ódio e não ódio. """

#Palavras que aparecem com alta frequência em ambos os casos

unwantedChars = [
    '\'', 
    '\"',
    '/p',
    'ª',
    'º',
    '.',
    '!',
    '?',
    ',',
    ';',
    ':',
    '-',
    '\'',
    '!!'
    ]

    

palavras = {
    'nao',
    'vai',
    'pra',
    'para',
    'ja',
    'sao',
    'ser',
    'la',
    'ai',
    'ta',
    'so',
    'ne',
    'la',
    'brasil',
    'carne'
}

#Como visualizar todos os dados da tabela
df = pd.DataFrame(data_frame) 
df

stopwords_2 = set(stopwords.words('portuguese')+list(punctuation)+ list(unwantedChars)+ list(palavras))

def removeStopWords2(text):
  #text.lower -> transforma todas as letras em minúculas
  #text ainda não foi tokenizado
  filteredWords = [word for word in word_tokenize(text.lower()) if word not in stopwords_2]
  return [word for word in filteredWords if len(word) > 1]

tokenized_data_frame_2 = data_frame[:]['Data'].apply(removeStopWords2)

tokenized_hate_data_frame_2 = hate_data_frame['Data'].apply(removeStopWords2)

tokenized_no_hate_data_frame_2 = no_hate_data_frame['Data'].apply(removeStopWords2)

df = pd.DataFrame(tokenized_data_frame_2)
df

"""**Gráficos**"""

#Palavras com maior frequência no dataframe retirando stopwords (pontuação + lista pt + lista pessoal de palavras)
allWords_data_final, frequencies_final = plotarNuvem(tokenized_data_frame_2)
plotFrenquencies(frequencies_final)

#Palavras com maior frequência no dataframe no hate retirando stopwords (pontuação + lista pt + lista pessoal de palavras)
allWords_data_nohate, freq_data = plotarNuvem(tokenized_no_hate_data_frame_2)
plotFrenquencies(freq_data)

#Palavras com maior frequência no dataframe hate retirando stopwords (pontuação + lista pt + lista pessoal de palavras)
allWords_data_hate, freq_data = plotarNuvem(tokenized_hate_data_frame_2)
plotFrenquencies(freq_data)

"""# Extraindo *Features*

**Bag of Words**

*Vocabulário* - lista de palavras que aparecem nos textos 

*Vetoriza-se os textos* - cada texto passa a ser descrito como um vetor que diz se uma determinada palavra *x* está inclusa ou não naquele texto
  - *CountVectorizer*: conta-se a quantidade de vezes que um determinado *token* aparece no texto. Contrói uma matriz esparsa com todos os x *tokens*.
  - TF-IDF *Vectorizer* (*term frequency-inverse document frequency*): é uma medida estatística que dá um peso para cada *token*, medindo a importância daquela palavra no documento. A importância aumenta porporcionalmente ao número de vezes que uma determinada palavra aparece no texto e é diminuida pela frequência dessa palavra no vocabulário. 

        TF(t) = número de vezes que t aparece no documento/número de palavras no documento
        IDF(t) = ln(número de documentos/número de documentos que contém t)
        TF-IDF(t) = TF*IDF

**Desvantagens**

  - O modelo *Bag of Words* não leva em consideração a ordem das palavras na frase. 

  - É preciso ser bastante cuidadoso ao se fazer o vocabulário, pois ele representará todo o mundo de palavras. 

  - Se existem muitas palavras no vocabulário, os vetores podem se tornar muito esparsos.
"""

#Construindo o vocabulário para o BoW

def extractNumericalBoW(frase, total_features_map):
  words = set(frase)
  word_features = {}

  for w in total_features_map:
    count = 0

    for word in words:
      if word == w:
        count = count+1

    word_features[w] = count

  return word_features

#lista com todas as palavras distintas 
total_features_map = list(frequencies_final.keys())[:]

#lista das classificações
Y_data = [label for label in data_frame[:]['Label']]
X_data = []

for i in range(len(allWords_data_final)):
   X_data.append((extractNumericalBoW(allWords_data_final[i], total_features_map), Y_data[i]))

# X_data é do tipo [({'votaram': 1, 'a' : 0, ....}, 'yes'), ({'votaram':0, 'a':1, ...}, 'no')]

allWords_data_final

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
import joblib

joblib.dump(total_features_map, "features_ddo_estudo.pkl")

"""# Classificação

## Busca dos melhores parâmetros

*Support Vector Machine* (SVM)


*  O aprendizado consiste na definição de hiperplanos que separam as classes do problema
*   Caso o problema seja não linear é preciso transformá-lo em um problema linear para que se possa utilizar um hiperplano para separar as classes. Para fazer isso usa-se os chamados de *kernel tricks* que adicionam uma nova dimensão ao problema, criando um novo atributo: *slack variable*.

*Random Forest*

*Naive Bayes*
"""

# kfold -> nº de partes que o dataset vai ser separado


def TrainClassifierWithScore (kfold, X_data_np, Y_data_np, classifier_type):

  scoring = ['accuracy', 'average_precision', 'f1', 'precision', 'recall', 'roc_auc']

  cv = KFold(n_splits = kfold, shuffle = True, random_state = 42)

  # C: default = 1, the strength of the regularization is inversely proportional to C
  # gamma: {'scale', 'auto'} ou float
  # 'auto' = 1/n_features
  # 'scale' = 1/(n_features*X.var())
  # class_weight: sets C -> class_weight[i]*C
  # 'balanced' = n_samples/(n_classes*np.bincount(y))
  #np.bincount -> Count number of occurrences of each value in array of non-negative ints

  if classifier_type == 'svm_poly':
    classifier = SVC(kernel='poly', C=0.45, probability=True, gamma='auto', class_weight='balanced')
      
  elif classifier_type == 'svm_rbf':
    classifier = SVC(kernel='rbf', C=0.45, probability=True, gamma='auto', class_weight='balanced')
      
  # Melhor configuração segundo o GridSearch -> buscando a melhor f1 score
  elif classifier_type == 'svm_linear_Grid':
    classifier = SVC(kernel = 'linear', C = 0.1, probability = True, gamma = 'auto', class_weight = 'balanced')

  elif classifier_type == 'svm_linear':
    classifier = SVC(kernel='linear', C=0.45, probability=True, gamma='auto', class_weight='balanced')

  elif classifier_type == 'naive_gaussiano':
    classifier = GaussianNB()

  elif classifier_type == 'naive_bernoulli':
    classifier = BernoulliNB(binarize = None)

  # Melhores parâmetros segundo o GridSearch
  elif classifier_type == 'random_forest_Grid':
    classifier = RandomForestClassifier(n_estimators = 200, criterion='gini', max_depth = 110, min_samples_leaf=1, min_samples_split=2 )

  #Parâmetros usados pelas meninas
  elif classifier_type == 'random_forest':
    classifier = RandomForestClassifier(n_estimators=100)

  results = cross_validate(estimator = classifier, X = X_data_np, y = Y_data_np, cv = cv, scoring = scoring, return_train_score = True)

  return results

def DictToArray(X_data):

  X_data_array = []
  Y_data_array = []

  for X in X_data:
    feature = []
    for feat in X[0].values():
      feature.append(feat)

    X_data_array.append(feature)
    Y_data_array.append(1 if X[1] == 'yes' else 0)

  X_data_np = np.array(X_data_array)
  Y_data_np = np.array(Y_data_array, dtype = int)

  return X_data_np, Y_data_array

X_data_np, Y_data_np = DictToArray(X_data)

"""Busca dos melhores hyperparâmetros através do GridSearch"""

kfold = 5

parametros = {
    'kernel': ['linear','poly','rbf'],
    'gamma': ['auto', 'scale'],
    'C': [0.1, 0.3, 0.45, 0.5, 0.6, 0.7, 0.8, 1],
    'class_weight' : ['balanced']
}

scoring = ['accuracy', 'average_precision', 'f1', 'precision', 'recall', 'roc_auc']

cv = KFold(n_splits = kfold, shuffle = True, random_state = 42)

grid = GridSearchCV(
    estimator = SVC(),
    param_grid = parametros,
    cv = cv,
    scoring = scoring,
    refit =  'f1',
    verbose = 3
)

grid.fit(X_data_np, Y_data_np)

print(grid.best_score_)
print(grid.best_params_)

results = grid.cv_results_
results = pd.DataFrame(results)
results

parametros_forest = {
    'n_estimators': [100, 200, 300],
    'criterion' : ['gini', 'entropy'],
    'max_depth' : [80, 90, 100, 110],
    'min_samples_split'  : [2, 5, 8, 10],
    'min_samples_leaf' : [1, 3, 5]
}

grid_forest = GridSearchCV(
    estimator = RandomForestClassifier(),
    param_grid = parametros_forest,
    scoring = scoring,
    refit = 'f1',
    cv = 5,
    verbose = 3
)

grid_forest.fit(X_data_np, Y_data_np)

print(grid_forest.best_score_)
print(grid_forest.best_params_)

results_forest = grid_forest.cv_results_

kfold = 5

#training SVM linear model
score_svm_linear = TrainClassifierWithScore(kfold, X_data_np, Y_data_np, 'svm_linear')

#training SVM linear model with GridSearch parameters
#score_svm_linear_grid = TrainClassifierWithScore(kfold, X_data_np, Y_data_np, 'svm_linear_Grid')

#training SVM polinomial model
score_svm_poly = TrainClassifierWithScore(kfold, X_data_np, Y_data_np, 'svm_poly')

#training Svm rbf model
score_svm_rbf = TrainClassifierWithScore(kfold, X_data_np, Y_data_np, 'svm_rbf')

#training Naive Bayes Gaussian Classifier
score_naive_gauss = TrainClassifierWithScore(kfold, X_data_np, Y_data_np, 'naive_gaussiano')

#training Naive Bayes Bernoulli Classifier
score_naive_bernoulli = TrainClassifierWithScore(kfold, X_data_np, Y_data_np, 'naive_bernoulli')

#training Random Forest model
score_random_forest = TrainClassifierWithScore(kfold, X_data_np, Y_data_np, 'random_forest' )

#training Random Forest model with GridSearch parameters
score_random_forest_Grid = TrainClassifierWithScore(kfold, X_data_np, Y_data_np, 'random_forest_Grid' )

#training SVM linear model with GridSearch parameters
score_svm_linear_grid = TrainClassifierWithScore(kfold, X_data_np, Y_data_np, 'svm_linear_Grid')

def printScoreTable(scores, kfold, classifier_type):
  print("\nClassifier:%s" % classifier_type)
  print("K:\tAcc.:\tPre:\tRec.:\tF1-S.:")
  mean_acc = 0
  mean_pre = 0
  mean_rec = 0
  mean_f1 = 0
  for i in range(kfold):
    mean_acc += (scores['test_accuracy'])[i]
    mean_pre += (scores['test_precision'])[i]
    mean_rec += (scores['test_recall'])[i]
    mean_f1 += (scores['test_f1'])[i]
    print("%i\t%.4f\t%.4f\t%.4f\t%.4f\t" % (i+1, scores['test_accuracy'][i], scores['test_precision'][i], scores['test_recall'][i], scores['test_f1'][i]))
  
  print("Mean\t%.4f\t%.4f\t%.4f\t%.4f\t" % (mean_acc/kfold, mean_pre/kfold, mean_rec/kfold, mean_f1/kfold))

#Teste Linear com os parâmetros das meninas
printScoreTable(score_svm_linear, kfold, 'svm_linear')

#Teste Linear com os parâmetros do GridSearch para melhor 'f1'
printScoreTable(score_svm_linear_grid, kfold, 'svm_linear_GridSearch')

#Teste RBF 
printScoreTable(score_svm_rbf, kfold, 'svm_rbf')

#Teste Poly 
printScoreTable(score_svm_poly, kfold, 'svm_poly')

#Teste Random Forest
printScoreTable(score_random_forest, kfold, 'random_forest')

#Teste Random Forest
printScoreTable(score_random_forest_Grid, kfold, 'random_forest_GridSearch')

#Naive Bayes Gaussiano
printScoreTable(score_naive_gauss, kfold, 'naive_bayes_gaussiano')

#Naive Bayes Bernoulli
printScoreTable(score_naive_bernoulli, kfold, 'naive_bayes_bernoulli')

"""## Fazendo o fit e as matrizes de confusão dos melhores modelos"""

#pd.Series ela numera o dado
# h = ['a', 'b', 'c']
# index  = [100, 101, 102]
# pd.Series(h, index=index)
# 100  'a'
# 101  'b'
# 102  'c'
y_data = pd.Series(Y_data_np)

from sklearn.model_selection import train_test_split
# 0,75 -> treino
x_train, x_test, y_train, y_test = train_test_split(X_data_np, y_data, random_state = 42)

#x_train, y_train 774
#x_test, y_test 259

classifier_svm = SVC(kernel = 'linear', C = 0.1, probability = True, gamma = 'auto', class_weight = 'balanced')
classifier_svm.fit(x_train,y_train)

classifier_naive = GaussianNB()
classifier_naive.fit(x_train, y_train)

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
import joblib

joblib.dump(classifier_svm, "SVM_linear_GridSearch_model.pkl")
joblib.dump(classifier_naive, "Naive_model.plk")

# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)  

# get the folder id where you want to save your file
file1 = drive.CreateFile()
file1.SetContentFile("SVM_linear_GridSearch_model.pkl")
file1.Upload()
file1 = drive.CreateFile()
file1.SetContentFile("Naive_model.plk")
file1.Upload()

import joblib
svm_linear_model = joblib.load('SVM_linear_GridSearch_model.pkl')
naive_model = joblib.load("Naive_model.plk")

from sklearn.metrics import confusion_matrix
y_predict_svm_linear = svm_linear_model.predict(x_test)
confusion_matrix(y_test, y_predict_svm_linear)

y_predict_naive = naive_model.predict(x_test)
confusion_matrix(y_test, y_predict_naive)

"""## Mas quais são as amostras erroneamente classificadas?"""

index = y_test.keys()
data = y_test.array

"""SVM Linear"""

true_positives_svm = []
true_negatives_svm = []
false_positives_svm = []
false_negatives_svm = []

for i in range (len(data)):
  if data[i] == y_predict_svm_linear[i]:
    if data[i] == 1:
      true_positives_svm.append(data_frame.Data[index[i]])
    else:
      true_negatives_svm.append(data_frame.Data[index[i]])
  if data[i] != y_predict_svm_linear[i]:
    if y_predict_svm_linear[i] == 1:
      false_positives_svm.append(data_frame.Data[index[i]])
    if y_predict_svm_linear[i] == 0:
      false_negatives_svm.append(data_frame.Data[index[i]])

tp_svm = pd.DataFrame(true_positives_svm)
fp_svm = pd.DataFrame(false_positives_svm)
fn_svm = pd.DataFrame(false_negatives_svm)
tn_svm = pd.DataFrame(true_negatives_svm)

tp_svm.to_csv('tp_svm.csv')
tn_svm.to_csv('tn_svm.csv')
fn_svm.to_csv('fn_svm.csv')
fp_svm.to_csv('fp_svm.csv')
!cp fn_svm.csv "drive/My Drive/"
!cp fp_svm.csv "drive/My Drive/"
!cp tp_svm.csv "drive/My Drive/"
! cp tn_svm.csv "drive/My Drive/"

"""Naive"""

true_positives_naive = []
true_negatives_naive = []
false_positives_naive = []
false_negatives_naive = []

for i in range (len(data)):
  if data[i] == y_predict_naive[i]:
    if data[i] == 1:
      true_positives_naive.append(data_frame.Data[index[i]])
    else:
      true_negatives_naive.append(data_frame.Data[index[i]])
  if data[i] != y_predict_naive[i]:
    if y_predict_naive[i] == 1:
      false_positives_naive.append(data_frame.Data[index[i]])
    if y_predict_naive[i] == 0:
      false_negatives_naive.append(data_frame.Data[index[i]])

tp_naive = pd.DataFrame(true_positives_naive)
fp_naive = pd.DataFrame(false_positives_naive)
fn_naive = pd.DataFrame(false_negatives_naive)
tn_naive = pd.DataFrame(true_negatives_naive)

tp_naive.to_csv('tp_naive.csv')
fn_naive.to_csv('fn_naive.csv')
fp_naive.to_csv('fp_naive.csv')
tn_naive.to_csv('tn_naive.csv')
!cp fn_naive.csv "drive/My Drive/"
!cp fp_naive.csv "drive/My Drive/"
!cp tp_naive.csv "drive/My Drive/"
!cp tn_naive.csv "drive/My Drive"

